{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7Aj4kE9dgm/CgITX1zqVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryavanshi18/Pytorch_Learnings/blob/main/Langchain_course_Coursera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ[\"OPENAI_API_KEY\"]=''"
      ],
      "metadata": {
        "id": "RFxgsYuPC9Md"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models Prompts Parsers"
      ],
      "metadata": {
        "id": "nbLsj2NvSQTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHm68oQqDKIf",
        "outputId": "0eaae08f-3b9c-47c1-d70d-4917a3488761"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFLkRC3XDJAd",
        "outputId": "676fb77a-bbf5-4a64-b699-969acb07bc84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.306-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.306 langsmith-0.0.41 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv,find_dotenv\n",
        "_=load_dotenv(find_dotenv(),override=True)"
      ],
      "metadata": {
        "id": "a3vqx9x0DS0U"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#openai.api_key=OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "ED_3pq2dEcNN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "aHFNnmktIz-a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "id": "4SEgqwPHEl_8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt,model=llm_model):\n",
        "  messages=[{\"role\":\"user\",\"content\":prompt}]\n",
        "  response=openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=0\n",
        "  )\n",
        "  return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "1nsq6CapHJRS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_completion(\"Hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vzrRCPLvKVRl",
        "outputId": "c74a9712-e9fe-4ce9-d9f6-e9c8f7495ef3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse,\\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "odvNJZOhKaB3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9M1ZnjZCgrP-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks\n",
        "into a style that is {style}.\n",
        "text:```{customer_email}```\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff4U16G2guIQ",
        "outputId": "2c6a5558-fa50-45de-e647-bc3b1902a8da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks\n",
            "into a style that is American English in a calm and respectful tone\n",
            ".\n",
            "text:```\n",
            "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=get_completion(prompt)"
      ],
      "metadata": {
        "id": "QhZ1X59vhKoa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zMVjadchhQ9o",
        "outputId": "cca621c4-e8f9-424d-be2b-5facf73d6628"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the expenses of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE2X4XTZhTTv",
        "outputId": "7ca859d2-fef3-4a86-db9a-22b826e442b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.305-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.305 langsmith-0.0.41 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "ruEjmeF-hb8Y"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat=ChatOpenAI(temperature=0.0,model=llm_model)\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5ZDxrSehf9A",
        "outputId": "f1c5830d-8c89-4f1a-8d90-165f9137f0cd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p6_gSXyQhowY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt_template=ChatPromptTemplate.from_template(template_string)\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCAjxvXgiHVI",
        "outputId": "b3c5a629-8279-4664-aa89-8ac8b60ff011"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['style', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1B2jfdZlbXC",
        "outputId": "7bbf0499-c222-41a8-909e-4e80b04096ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXDhvd_7luB6",
        "outputId": "bb3c936b-cc00-497b-bea5-a29cc8fc9f35"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# template = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "#     (\"human\", \"Hello, how are you doing?\"),\n",
        "#     (\"ai\", \"I'm doing well, thanks!\"),\n",
        "#     (\"human\", \"{user_input}\"),\n",
        "# ])\n",
        "\n",
        "# messages = template.format_messages(\n",
        "#     name=\"Bob\",\n",
        "#     user_input=\"What is your name?\"\n",
        "# )"
      ],
      "metadata": {
        "id": "2mvQkzmVlLOY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNyITkfwlL6l",
        "outputId": "0c0a3547-12ca-4af0-f419-2cc7676caad5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
              " HumanMessage(content='Hello, how are you doing?'),\n",
              " AIMessage(content=\"I'm doing well, thanks!\"),\n",
              " HumanMessage(content='What is your name?')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CAE6Sp_DlM31"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email )"
      ],
      "metadata": {
        "id": "hteNo7nGl4Fu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(customer_messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yAGr5bMndvU",
        "outputId": "25478c2e-8216-4110-e7e8-25f40436660f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(customer_messages[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcmKQSIynhp6",
        "outputId": "04cc5307-ed8e-4aca-fe1b-21de8b101ecf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain.schema.messages.HumanMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response=chat(customer_messages)\n",
        "print(customer_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ukAt7KnjUm",
        "outputId": "96a660a4-9518-4afb-a426-da0412207cfe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, my friend.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9OX6f1pnuHH",
        "outputId": "21d8f3ce-0088-4ef2-e251-a63299bb8d13"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, my friend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_IH_U3Emn2Xd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_style_pirate = \"\"\"\\\n",
        "a polite tone \\\n",
        "that speaks in English Pirate\\\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KbSNvsE1n7N1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply)"
      ],
      "metadata": {
        "id": "rpwptTPYn8l0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_response = chat(service_messages)"
      ],
      "metadata": {
        "id": "0w__MAKNn_NN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zqNz9_ENoAtt",
        "outputId": "b77aad77-2951-45f6-da8e-8b16fd952180"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ahoy there, matey! I regret to inform ye that the warranty be not coverin' the costs o' cleanin' yer galley, as 'tis yer own fault fer misusin' yer blender by forgettin' to secure the lid afore startin' it. Aye, tough luck, me heartie! Fare thee well!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"gift\":False,\n",
        "    \"delivery_days\":5,\n",
        "    \"price_value\":\"pretty affordable!\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Qq5maKoFfj",
        "outputId": "31188b52-7eb2-49d3-e210-0c99e33d7713"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6Hg5ZhTPpW19"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template= ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu6nuJNFphn8",
        "outputId": "dc6d455a-261d-49c5-f051-9c57710903ba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "response = chat(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h81SRSEfppgF",
        "outputId": "b5c3a9f2-31d2-4a44-f2c6-9bde57a3652f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": false,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBcYH7nVPTn5",
        "outputId": "bc0fcad0-c7df-43de-e509-81014a4acde9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ],
      "metadata": {
        "id": "u2-p_Sv0Pdc0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gift_schema=ResponseSchema(name='gift',\n",
        "                           description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
        "                                      description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\",\n",
        "                                    description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema,\n",
        "                    delivery_days_schema,\n",
        "                    price_value_schema]"
      ],
      "metadata": {
        "id": "Z92Yl3n1PgF9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "m-67tfIGPwze"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "V-XskRClPzGg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYubmgJMP0yK",
        "outputId": "7464be27-83f5-43d4-c4b1-409d95843d09"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Memory\n",
        "## Outline\n",
        "* ConversationBufferMemory: This memory allows for storing of messages and then extracts the messages in a variable<br>\n",
        "* ConversationBufferWindowMemory: Keeps a list of last k conversation<br>\n",
        "* ConversationTokenBufferMemory: Keeps a list of conversations depending on token length <br>\n",
        "* ConversationSummaryMemory: Creates summary over time"
      ],
      "metadata": {
        "id": "3PoipDH1SjoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "AHlT2BI6SlV5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# account for deprecation of LLM model\n",
        "import datetime\n",
        "# Get the current date\n",
        "current_date = datetime.datetime.now().date()\n",
        "\n",
        "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
        "target_date = datetime.date(2024, 6, 12)\n",
        "\n",
        "# Set the model variable based on the current date\n",
        "if current_date > target_date:\n",
        "    llm_model = \"gpt-3.5-turbo\"\n",
        "else:\n",
        "    llm_model = \"gpt-3.5-turbo-0301\""
      ],
      "metadata": {
        "id": "Sp3IP0hXHxYL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmp8TckVH1mS",
        "outputId": "3ee6437d-a6fe-4712-f6f4-b82cb161d2d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.305-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.305 langsmith-0.0.41 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "YKY9_25SHzX6"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "56EknLUPH6aC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "mSfiVL1RIjjH",
        "outputId": "8b3f9914-1a20-4f7d-87c9-568f40d5d2ca"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Andrew\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "0qBv8ENKLWdz",
        "outputId": "226f4f02-e93d-4bb0-f9a6-c89508ba08e5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 is equal to 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "-LMTccZxLZhQ",
        "outputId": "909e35a9-93ec-4a91-aab0-024dfee0b8d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Andrew.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODNwC9VtLcUJ",
        "outputId": "39dcbc32-c8a4-48a0-c9c7-4ae509580e09"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is my name?\n",
            "AI: Your name is Andrew.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR72kppnLfsD",
        "outputId": "7f18f733-5b7b-4be2-a8a9-69385574505d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ],
      "metadata": {
        "id": "bXcsy3SdLiOI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvDna3aCNdY-",
        "outputId": "93c064e8-0206-4d99-d131-522182c8e6c1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\\nHuman: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ],
      "metadata": {
        "id": "B5QzZsVaNfCm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ksjF3tNkPP",
        "outputId": "f9452756-0623-4919-ab23-065129c88f51"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\\nHuman: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "QnSKpx2kNl4A"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "#remember only last k context in the memory"
      ],
      "metadata": {
        "id": "a4VsK-gqRJHJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ],
      "metadata": {
        "id": "8Vx8TvSeR2dB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r65kf70HSH2J",
        "outputId": "c78db43c-af19-4ce8-f135-87a5d8a87cca"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(temperature=0.0,\n",
        "               model=llm_model)\n",
        "memory=ConversationBufferWindowMemory(k=1)\n",
        "conversation=ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "HmpDLIO0SJba"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QUhDIYcfSinJ",
        "outputId": "c9871604-79d7-4c6e-a61f-bdebbf885002"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rN4P95e-TBeW",
        "outputId": "d0609ee9-2d4d-4f2f-f525-4055349d2c81"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 is equal to 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YjiE9NN0SmIF",
        "outputId": "024de395-7455-484f-b71e-b2cf6f357c81"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since k=1, it can't remember the answer for input What is my name?"
      ],
      "metadata": {
        "id": "HKkzAnK9SvVD"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz_75vtTVR3-",
        "outputId": "1f4d33fd-fd77-485e-cd42-361d142b1653"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm=ChatOpenAI(temperature=0.0,model=llm_model)"
      ],
      "metadata": {
        "id": "ujDkb4_ZTQos"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory=ConversationTokenBufferMemory(llm=llm,max_token_limit=30)\n",
        "#Different llms have different token limit, we can customize it over here\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ],
      "metadata": {
        "id": "mx-kanF5U-Iq"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvJhOHeFVNW3",
        "outputId": "4313b50a-450c-4e2c-b3c9-b2b97f66adb4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "7UTgXtdQVtFE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ],
      "metadata": {
        "id": "j6WgPLV2WTSH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7eH7vpqWWO8",
        "outputId": "80c1e5e1-6746-417e-f2f9-9b8d55e16ff5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "96rAfPg3WZOO"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "m9pu90FyWbQ_",
        "outputId": "4b37fa8b-4252-48d0-d812-d4950c161d6f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of AI in natural language processing and generate interest in potential applications for the customer's business.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [['Queen Size Sheet Set', 'I ordered a king size set. My only criticism would be that I wish seller would offer the king size set with 4 pillowcases. I separately ordered a two pack of pillowcases so I could have a total of four. When I saw the two packages, it looked like the color did not exactly match. Customer service was excellent about sending me two more pillowcases so I would have four that matched. Excellent! For the cost of these sheets, I am satisfied with the characteristics and coolness of the sheets'],\n",
        " ['Waterproof Phone Pouch', 'I loved the waterproof sac, although the opening was made of a hard plastic. I don’t know if that would break easily. But I couldn’t turn my phone on, once it was in the pouch.'],\n",
        "        ['Luxury Air Mattress', 'This mattress had a small hole in the top of it (took forever to find where it was), and the patches that they provide did not work, maybe because it''s the top of the mattress where it''s kind of like fabric and a patch won''t stick. Maybe I got unlucky with a defective mattress, but where''s quality assurance for this company? That flat out should not happen. Emphasis on flat. Cause that''s what the mattress was. Seriously horrible experience, ruined my friend''s stay with me. Then they make you ship it back instead of just providing a refund, which is also super annoying to pack up an air mattress and take it to the UPS store. This company is the worst, and this mattress is the worst.'],\n",
        "  ['Pillows Insert','This is the best throw pillow fillers on Amazon. I’ve tried several others, and they’re all cheap and flat no matter how much fluffing you do. Once you toss these in the dryer after you remove them from the vacuum sealed shipping material, they fluff up great'],\n",
        "   ['Milk Frother Handheld','\\xa0I loved this product. But they only seem to last a few months. The company was great replacing the first one (the frother falls out of the handle and can''t be fixed). The after 4 months my second one did the same. I only use the frother for coffee once a day. It''s not overuse or abuse. I''m very disappointed and will look for another. As I understand they will only replace once. Anyway, if you have one good luck.']]"
      ],
      "metadata": {
        "id": "nQ4qMIybfRJ7"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umcz0tnJhyyj",
        "outputId": "103037a2-7950-4278-cda4-d2483d229aee"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data, columns=['Product', 'Review'])"
      ],
      "metadata": {
        "id": "KrVJ_edUhrIW"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "h8L7JARUhlSk"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FPC9JrubhmJY",
        "outputId": "d6f50889-9fb8-4a58-da89-8d618a539c60"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Product                                             Review\n",
              "0    Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
              "1  Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
              "2     Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
              "3          Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
              "4   Milk Frother Handheld   I loved this product. But they only seem to l..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88bc8567-6088-4137-9582-1b37f387d765\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88bc8567-6088-4137-9582-1b37f387d765')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88bc8567-6088-4137-9582-1b37f387d765 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88bc8567-6088-4137-9582-1b37f387d765');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcfca459-2d4b-43d5-b950-96fb16f99986\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcfca459-2d4b-43d5-b950-96fb16f99986')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcfca459-2d4b-43d5-b950-96fb16f99986 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(temperature=0.9,model=llm_model)\n",
        "prompt=ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes a {product}?\"\n",
        ")"
      ],
      "metadata": {
        "id": "IkBup7iKhqQL"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=LLMChain(llm=llm,prompt=prompt)"
      ],
      "metadata": {
        "id": "rIOw009_iMjZ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product=\"Queen Size Sheet Set\""
      ],
      "metadata": {
        "id": "NG_qvpWfiScq"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LgGZMoFWiWU-",
        "outputId": "e4b1fe9d-c3b1-47e1-c209-4510282c6c2d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Royal Comfort Linens'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain#Single input output"
      ],
      "metadata": {
        "id": "TKoXeOJviYHP"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(temperature=0.9,model=llm_model)\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "chain_one=LLMChain(llm=llm,prompt=prompt)\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "chain_two=LLMChain(llm=llm,prompt=second_prompt)"
      ],
      "metadata": {
        "id": "2eHyHHvPivQJ"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain=SimpleSequentialChain(chains=[chain_one,chain_two],\n",
        "                                           verbose=True)\n"
      ],
      "metadata": {
        "id": "cH9KD1bHjDDG"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "nMeFC6LIjOP_",
        "outputId": "5823b5b8-9810-4da1-ce76-df6077caa282"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mRoyal Comfort\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mRoyal Comfort is a premium bedding company that offers luxurious and comfortable bedding products for a peaceful sleep experience.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Royal Comfort is a premium bedding company that offers luxurious and comfortable bedding products for a peaceful sleep experience.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain #Multi Input Outputs"
      ],
      "metadata": {
        "id": "tz8L7JBejRGM"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(temperature=0.9,model=llm_model)\n",
        "first_prompt=ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\")\n",
        "chain_one=LLMChain(llm=llm,prompt=first_prompt,\n",
        "                   output_key=\"English_Review\")"
      ],
      "metadata": {
        "id": "XNdxn1ubl0iN"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt=ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "chain_two=LLMChain(llm=llm,prompt=second_prompt,\n",
        "                   output_key=\"summary\")\n"
      ],
      "metadata": {
        "id": "22xIqYeoma0b"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )"
      ],
      "metadata": {
        "id": "pGmeq7QFm0_i"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )\n"
      ],
      "metadata": {
        "id": "eM-Ify-Jm32W"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "xIX6zHiZm5oe"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.Review[3]\n",
        "overall_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAcznzjTm7Bx",
        "outputId": "c50ab38c-cfb9-4453-d74c-3f7e116c7a71"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-iPRiUaTkPDK2FWStEJcv7gto on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-iPRiUaTkPDK2FWStEJcv7gto on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-iPRiUaTkPDK2FWStEJcv7gto on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-iPRiUaTkPDK2FWStEJcv7gto on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': 'This is the best throw pillow fillers on Amazon. I’ve tried several others, and they’re all cheap and flat no matter how much fluffing you do. Once you toss these in the dryer after you remove them from the vacuum sealed shipping material, they fluff up great',\n",
              " 'English_Review': 'This is the best throw pillow fillers available on Amazon. I have tried several others, and they all feel cheap and flat no matter how much I fluff them. However, once you put these in the dryer after removing them from the vacuum-sealed packaging, they fluff up wonderfully.',\n",
              " 'summary': 'The reviewed throw pillow fillers available on Amazon outperform other options as they become fluffy and maintain their shape, unlike other cheaper alternatives that remain flat regardless of fluffing.',\n",
              " 'followup_message': \"Thank you for providing this insightful summary. It's great to hear that the throw pillow fillers available on Amazon have been reviewed positively, as they consistently outperform other options by maintaining their shape and fluffiness. This sets them apart from cheaper alternatives that tend to remain flat even after fluffing. Your feedback emphasizes the value and quality these fillers offer.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "dcSqEA8Jm_lH"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "J7cYb11XzvKb"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "19NT-ATPzxae"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ],
      "metadata": {
        "id": "tYo7SXWZz0b6"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "81P_kO7az22g"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "7L9EOd68z42x"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "vfUDVN9Tz7a-"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "G7iTrp0Pz92C"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "9i4e9FHgz_5l"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "2DgDq_SF0Bmp",
        "outputId": "1fa098d4-5f31-456b-d377-1e71e0c79ca7"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called \"black body\" because it absorbs all wavelengths of light, appearing black at room temperature. \\n\\nAccording to Planck\\'s law, black body radiation is characterized by a continuous spectrum of wavelengths and intensities, which depend on the temperature of the object. As the temperature increases, the peak intensity of the radiation shifts to shorter wavelengths, resulting in a change in color from red to orange, yellow, white, and eventually blue at very high temperatures.\\n\\nBlack body radiation is a fundamental concept in physics and has various applications, including understanding the behavior of stars, explaining the cosmic microwave background radiation, and developing technologies like incandescent light bulbs and thermal imaging devices.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "dumQKRWG0C5S",
        "outputId": "9a6f57b2-807a-4795-b634-7ee068ec4ba3"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Thank you for your kind words! As a mathematician, I am happy to help with any math questions, no matter how simple or complex they may be.\\n\\nNow, let's solve the problem at hand. The question is asking for the sum of 2 and 2. To find the answer, we can simply add the two numbers together:\\n\\n2 + 2 = 4\\n\\nTherefore, the sum of 2 and 2 is 4.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Why does every cell in our body contain DNA?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "3T0m18390LXZ",
        "outputId": "fefe686b-f42e-46db-994a-7f1510e60808"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-iPRiUaTkPDK2FWStEJcv7gto on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None: {'input': 'Why does every cell in our body contain DNA?'}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-iPRiUaTkPDK2FWStEJcv7gto on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for the development, functioning, and reproduction of all living organisms. DNA contains the information necessary for the synthesis of proteins, which are essential for the structure and function of cells. It serves as a blueprint for the production of specific proteins that determine the characteristics and traits of an organism. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next during reproduction. Therefore, every cell in our body contains DNA to ensure the proper functioning and continuity of life.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gpt-3.5-turbo-0301\""
      ],
      "metadata": {
        "id": "_053qz3G0PnX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "xRVYxxU6RH13"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=CSVLoader('OutdoorClothingCatalog_1000.csv')"
      ],
      "metadata": {
        "id": "nKpG745GRJsC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "MT7YguXEVk_6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install docarray\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhL2HlYkVn9Y",
        "outputId": "4b935ba5-8af9-4d52-eda9-25bbcbf5d22b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation.qa import QAEvalChain\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
        "eval_chain = QAEvalChain.from_llm(llm)\n",
        "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
        "for i, eg in enumerate(examples):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(\"Question: \" + predictions[i]['query'])\n",
        "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
        "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
        "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
        "    print()"
      ],
      "metadata": {
        "id": "E1pf2H6rWSdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}